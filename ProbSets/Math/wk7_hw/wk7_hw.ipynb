{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Multivariable Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we discussed Gradient Descent, Newton's, Broyden's, Conjugate Gradient, and Nelder-Mead method for multivariable, nonlinear optimization.  \n",
    "\n",
    "Steepest Descent finds optimum by doing a line search in the direction of the gradient. It can solve most differentiable functions but yields poor results for non-differentiable ones. It is relatively accurate, but also computationally expensive because of the 'zig-zagging' nature of the derivative. \n",
    "\n",
    "Newton's method approximates a function that we wants to minimize by its second-degree taylor polynomial. To find the minimizer of the orginal function, it finds the minimizer $x_{k+1}$ of the taylor polynomial around $x_k$, and check for convergence. Newton's method may fail for functions whose roots do not have a second derivativ, by 'overshooting' away from the optimizer. It converges very fast (quadratically), but relies on a good initial guess, and can get difficult if the hessian is hard to calculate.\n",
    "\n",
    "Broyden's method is similar to Newtoin's. Instead of using the actual Hessian matrix, it uses some positive-definite approximation of $D^2f(x_0)$. Broyden's method is useful for functions whose second-derivatives are hard to calculate. However, since we have to calculate a rank-one approximation at each step, which involves inverting the approximation matrix from last step, it is still computationally complicated.  \n",
    "\n",
    "Conjugate Gradient searches in the direction of the Q-conjugates of a quadratic function until convergence. It is useful when the Hessian is hard to calculate (so each step of Newton's Method is expensive).\n",
    "\n",
    "Nelder-Mead is different from the other methods in that it does not use gradient. Instead, it finds the optimum by calculating the best, worst, and second worst points in a given region. Through comparison of these points with the reflection, contraction, or expansion, it then changes the region until convergence. Nelder-Mead is more likely to yield a solution, yet it may converge to stationary points so depends a lot on initial guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def steepest_descent(x0, b, Q, episilon=1e-6):\n",
    "    Df = lambda x: (1/2) * np.dot(x.T, Q.T + Q) - b.T \n",
    "    i = 0\n",
    "    max_iter = 1000\n",
    "    D = 1\n",
    "    \n",
    "    while i < max_iter and D > episilon:\n",
    "        i += 1\n",
    "        x1 = np.copy(x0)\n",
    "        x0 = (np.outer(Df(x1), Df(x1).T))/(np.dot(Df(x1, Q), Df(x1).T))\n",
    "        D = np.abs(x0)\n",
    "        \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Df Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Df(f, x, Rerr):\n",
    "    \n",
    "    h = np.sqrt(Rerr)\n",
    "    e = np.zeros_like(x)\n",
    "    Df = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        e[i] = h \n",
    "        Df[i] = (f(x + e) - f(x))/h\n",
    "        \n",
    "    return Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quadratic function, \n",
    "$$f'(x) = Qx - b$$\n",
    "$$f''(x) = Q$$\n",
    "\n",
    "$\\forall$ initial guess $x_0$,  we know that the result of one iteration of Newton's Method is $x_1 =x_0 - D^2f(x_0)^{-1}Df(x_0)^T=x_0 - Q^{-1}(Qx_0-b) = Q^{-1}b$. So $x_1$ is a critical value of the function. Moreover, since $Q$ (the Hessian) is positive definite, $x_1$ must be a minimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
